<p class="c1 c26"><span class="c3"></span></p><h4 class="c56" id="h.8jif9qmjh3se"><span class="c13 c54">Kevin B Korb, Steven Mascaro, Erik P Nyberg and Yang Li (Kelvin) </span></h4><p><span class="c3">6 Nov 2022</span></p><p class="c35 c26"><span class="c3"></span></p><h3 class="c11" id="h.nglbbo2fi7i5"><span class="c14">What Is Causal Attribution?</span></h3><p class="c33"><span>While </span><span>there</span><span>&nbsp;is a long tradition in science and philosophy (and in ordinary language) of treating causes as necessary conditions in a deterministic world, modern jurisprudence, science and philosophy have accepted that causes are probability-raising events in a stochastic (non-deterministic) world and that attribution of a cause to an effect will never be entirely exclusive of other factors. Lung cancers cannot be ascribed necessarily or exclusively to cigarette smoking, for example, but that doesn&rsquo;t stop the reasonable conclusion of a smoker&rsquo;s history being a primary cause in many cases. Similarly, methods for attributing extreme weather events to anthropogenic global warming have been developed and gained support in the climate sciences (see Korb, 2020), despite weather being an uncertain result of </span><span>climate</span><span>. </span><span class="c31">Demands to demonstrate a deterministic connection for particular events before attributing any causal blame are unreasonable.</span><span class="c3">&nbsp;</span></p><p class="c33"><span class="c3">CAT (the Causal Attribution Tool) is a tool for exploring different causal scenarios incorporating uncertainties and allowing you to entertain and test different hypotheses about what is causing what in those scenarios. CAT will also allow you to compare alternative ideas about the nature of causal attribution itself; that is, distinct criteria for judging causal attribution can (and will) be implemented &ndash; although we have implemented our own preferred criterion first, naturally, along with a couple of popular alternatives.</span></p><p class="c33"><span>The basis for this tool is the use of </span><span class="c18 c9"><a class="c8" href="https://www.researchgate.net/publication/227088133_The_Causal_Interpretation_of_Bayesian_Networks">causal Bayesian networks</a></span><span class="c3">&nbsp;(CBNs) to portray causal scenarios.</span></p><p class="c35"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 324.22px; height: 336.92px;"><img alt="" src="images/image1.png" style="width: 324.22px; height: 336.92px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h3 class="c11" id="h.khsjdda6op8m"><span class="c14">Causal Bayesian Networks</span></h3><p class="c1"><span>A causal Bayesian network is a graph whose nodes represent the important factors, causes and effects, in a network system where arrows represent the direct causal influences of the parent (cause) on the child (effect). &nbsp;Thus, in the CBN displayed above, Judea Pearl&rsquo;s home alarm network (Pearl, 1988), the Alarm sounding is directly connected to everything: &nbsp;burglaries and earthquakes directly cause the Alarm to sound and in turn that will directly cause Pearl&rsquo;s neighbors to call and warn him at work when it does sound. &nbsp;The connections cause changes in the effect node&rsquo;s probability distribution in particular circumstances. Thus, for example, when there is neither an earthquake nor a burglary Pearl&rsquo;s Alarm will sound with probability 0.001 regardless (as indicated by the line &ldquo;P(A|~B,~E) = 0.001&rdquo;) [for notation, see box below], perhaps due to some additional unrepresented cause, such as wind, which is not explicitly represented in the model. (</span><span>Causal models</span><span>&nbsp;do not have to be complete in order to be useful; in fact, there is no such thing as a complete model in science [consider George Box&rsquo;s &ldquo;</span><span class="c18"><a class="c8" href="https://en.wikipedia.org/wiki/All_models_are_wrong">All models are wrong</a></span><span class="c3">&rdquo;].)</span></p><p class="c1 c26"><span class="c3"></span></p><a id="t.e7d2cbc7d4c7dafaec02e39dfb0f3fcc432d403e"></a><a id="t.0"></a><table class="c29"><tr class="c15"><td class="c45" colspan="1" rowspan="1"><p class="c25"><span class="c24 c17 c30">Notation</span></p><p class="c25 c26"><span class="c30 c24 c17"></span></p><p class="c12"><span class="c17 c24">P(A) = 0.05</span><span class="c19">&nbsp;is a simple probability statement, asserting that the event A has a 5% chance of being true. For example, if A stands for rain occuring today, then the probability statement stands for that having a 1 in 20 chance.</span></p><p class="c12"><span class="c24 c17">P(A|B) = 0.33</span><span class="c19">&nbsp;is a conditional probability statement, asserting that if we come to know that B is true, then A will have a &#8531; chance of being true. For example, if B is a die rolling odd, and A is the die rolls a 1, then this statement will be true.</span></p><p class="c12"><span class="c24 c17">P(~A|B) = 0.67</span><span class="c24 c42">&nbsp;is the conditional probability statement asserting that the probability of A </span><span class="c24 c17">not</span><span class="c19">&nbsp;occurring is 0.67.</span></p><p class="c12"><span class="c24 c17">P(A|B,C) = 0.17 </span><span class="c24 c42">is the conditional probability statement asserting that if </span><span class="c24 c17">both</span><span class="c19">&nbsp;B and C are true, then A has the probability 0.17.</span></p><p class="c12"><span class="c24 c42">Placing two probabilities together, such as </span><span class="c24 c17">P(A)P(B) </span><span class="c24 c42">means to multiply them, i.e., it&rsquo;s the same as </span><span class="c24 c17">P(A) X P(B).</span></p><p class="c12"><span class="c10">&sum;</span><span class="c24 c42">&nbsp;is the sum operator. For example, </span><span class="c10">&sum; a</span><span class="c44 c17">i </span><span class="c24 c42">is the sum </span><span class="c10">a</span><span class="c44 c17">0 </span><span class="c10">+ a</span><span class="c44 c17">1 </span><span class="c10">+ a</span><span class="c17 c44">2 </span><span class="c10">+ &hellip; </span><span class="c24 c42">for however many values </span><span class="c44 c17">i </span><span class="c24 c42">takes. The statement </span><span class="c24 c17">i &#8714; K</span><span class="c24 c42">, in particular, would mean </span><span class="c24 c17">i </span><span class="c24 c42">takes all the possible values in a set </span><span class="c24 c17">K.</span></p><p class="c5"><span class="c3"></span></p></td></tr></table><p class="c1 c26"><span class="c3"></span></p><p class="c1"><span>It doesn&rsquo;t take a sophisticated causal attribution theory to see that earthquakes can cause Pearl&rsquo;s Alarm to go off. Where theories differ lies in such questions as </span><span class="c17">how much</span><span class="c3">&nbsp;responsibility to put on one cause versus another cause, especially when causal influences are mediated by a complex subnetwork of connections to an effect and when they are being considered in distinct background contexts.</span></p><p class="c1"><span>Here we shall briefly describe our preferred account of causal attribution, based upon information theory. However, this tool allows for other accounts to be represented and compared with our own and with each other &ndash; at least in principle by adding such criteria to our tool (which is open source code). All of them will need to be implemented as tests on the properties of causal Bayesian networks, to be sure. Hence, they will all have to be based upon (or representable within the framework of) the interventionist theory of causality, which identifies causality with &ldquo;difference making&rdquo; in a CBN, as we have described above. (For interventionist theories of causality see Judea Pearl&rsquo;s </span><span class="c21"><a class="c8" href="https://books.google.co.nz/books?id%3DLLkhAwAAQBAJ%26printsec%3Dfrontcover%26dq%3Dpearl%2Bcausality%26hl%3Den%26sa%3DX%26ved%3D0ahUKEwjd67Px9vTlAhWzheYKHdosBngQ6AEIKTAA%2522%2520%255Cl%2520%2522v%3Donepage%26q%3Dpearl%2520causality%26f%3Dfalse">Causality</a></span><span>&nbsp;(Pearl, 2000)</span><span>, James Woodward&rsquo;s </span><span class="c21"><a class="c8" href="https://books.google.co.nz/books?hl%3Den%26lr%3D%26id%3D-H08DwAAQBAJ%26oi%3Dfnd%26pg%3DPR7%26dq%3Dwoodward%2Bmaking%2Bthings%2Bhappen%26ots%3D78NMnV__Qi%26sig%3D-YoN7LqOvM7pku_og9vwfOTQcAA%26redir_esc%3Dy%2522%2520%255Cl%2520%2522v%3Donepage%26q%3Dwoodward%2520making%2520things%2520happen%26f%3Dfalse">Making Things Happen</a></span><span>&nbsp;(Woodward, 2005),</span><span>&nbsp;or our own </span><span class="c55"><a class="c8" href="https://www.semanticscholar.org/paper/The-Metaphysics-of-Causal-Models-Handfield-Twardy/c41e2527da2254277d7dd61ed7861fff6f7964f2">&ldquo;The Metaphysics of Causal Models,&rdquo; </a></span><span class="c21"><a class="c8" href="https://www.semanticscholar.org/paper/The-Metaphysics-of-Causal-Models-Handfield-Twardy/c41e2527da2254277d7dd61ed7861fff6f7964f2">Erkenntnis</a></span><span>&nbsp;(Handfield et al., 2008).</span><span class="c3">)</span></p><h3 class="c11" id="h.261uayhi6jcx"><span class="c14">Causal Information Theory</span></h3><p class="c1"><span>Causal Information Theory is the basis for our own preferred method for attributing causation. General information theory studies the relations between probabilistic variables, with </span><span class="c18"><a class="c8" href="https://en.wikipedia.org/wiki/Mutual_information">Mutual Information</a></span><span>&nbsp;between two variables &ndash; written as </span><span class="c39">I(C,E)</span><span class="c3">&nbsp;for variables C and E &ndash; being the key concept, measuring the probabilistic dependency between any two variables (or nodes, in a CBN) C and E as:</span></p><p class="c35"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 361.22px; height: 75.73px;"><img alt="" src="images/image4.png" style="width: 361.22px; height: 75.73px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c3">Two key features of Mutual Information are that it is non-negative, with zero being a minimum, indicating no probabilistic dependency between the variables at all, and it is symmetric. The degree of Mutual Information tells us how much information about E is carried by C (and vice versa). So, the non-negativity of Mutual Information means that learning the value of one variable can never be expected to reduce our information about another. The maximal Mutual Information with a variable E is carried by E itself, the so-called self-information I(E,E), which is also known as the entropy of E.</span></p><p class="c1"><span class="c3">Thus, self-information identifies the maximum amount that a cause can be responsible for the state of the effect; so we use that amount in CAT to ascribe degrees of responsibility borne by different causes of an effect. In effect, we use self-information to establish a 100% limit to how much any cause can influence the given effect, so as to provide a common scale for causal attributions across different scenarios. Thus, causes which reduce the entropy of E (provide information about E) are reported to have a positive % effect. (To provide a consistent standard for comparison when looking at these percentages of explained variance in CAT, we consider the self-information of E when it has a uniform distribution, that is to say, when it takes its maximum entropy.) &nbsp;Given that Mutual Information is non-negative, no causal variable has a negative % effect, increasing the entropy of E. </span></p><p class="c1"><span>There is a rich tradition in information theory; those wishing to learn about it can look at the accessible and excellent textbook by Cover and Thomas (2006). Its value for understanding causation lies in the </span><span class="c18"><a class="c8" href="https://plato.stanford.edu/archives/spr2021/entries/causation-probabilistic">theory of probabilistic causality</a></span><span>, which is the origin of interventionist accounts of causality. Probabilistic causality asserts that our world can be described by a network of stochastic causal relations, which gives rise to probabilistic dependencies. Mutual Information allows us to </span><span>measure the strength of </span><span class="c3">those probabilistic relations, but we need something else to measure the underlying causal relations. Simply trying to replace causality with probabilistic dependencies is notoriously wrong-headed, leading to such absurdities as trying to get tenure at an academic institution by bleaching your hair white, or trying to avoid lung cancer by preventing tobacco stains on your fingers with a cigarette holder. The probabilistic dependencies between those kinds of events are real, but the causal dependencies are spurious. How can we understand that both dependency claims are true &ndash; i.e., that the probabilistic dependency holds, while the causal dependencies do not?</span></p><p class="c1"><span class="c3">We can understand these statements being compatible with a simple causal Bayesian network; e.g., one for the tenure decision might look like this:</span></p><p class="c35"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 437.22px; height: 219.60px;"><img alt="" src="images/image2.png" style="width: 437.22px; height: 219.60px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span>Assuming the arrows represent positive causal relations, such a causal model will induce a positive correlation between having White Hair and getting Tenure: the common cause of Age/Time sets up such a probabilistic dependency. However, causality itself is asymmetrical: later effects cannot bring about changes in earlier causes. So, bleaching your hair white will have no effect on a tenure committee&rsquo;s decision (except, perhaps, in indicating how silly you are, but that reaches for variables beyond the model here). In Pearl&rsquo;s &ldquo;Do-Calculus&rdquo; (Pearl, 2000), and related interpretations of causal Bayesian networks, we can think </span><span>causally</span><span class="c3">, rather than probabilistically, by representing causal interventions by cutting all directly inbound arrows on the cause under consideration. For example, if we &ldquo;set&rdquo; White Hair = True after cutting the arrow inbound from Age/Time, we can then (and only then) examine the probability distribution over Tenure to see if the chances of a tenure approval have changed. They will not have. Thus, causal Bayesian networks can represent both the observed probabilistic dependencies and the unobserved causal dependencies of real causal systems. These unobserved causal relations are only unobserved if you are content with observational (sample) data: if you go to the trouble of intervening on the systems, as in running randomized experiments for example, then you can see the causal dependencies clearly enough. In any case, causal modeling, and our causal attribution tool, give us access to both views.</span></p><p class="c1"><span>In short, our concept of Causal Information is identical to Mutual Information, except that it is measured in a causal Bayesian network which has undergone a causal intervention. </span><span>(Note: This is a significant exception; while the formula above applies to both CI and MI, they are applied in very different circumstances, and you can&rsquo;t expect a CI measure to be identical to some MI measure reported by CAT.) </span><span>What intervention to perform and what should show up in the final Causal Information measure depends upon exactly what the question about causal attribution is. For example, one can use the very same CBN of Pearl&rsquo;s Alarm network to answer the question, </span><span>&ldquo;Did the Earthquake cause Mary to call Judea?&rdquo;</span><span class="c3">&nbsp;on a typical day (with, however, an earthquake) and on an atypical day when John also called Judea to report his Alarm going off (i.e., with two calls to Judea). There are many other causal questions that might be raised and answered as well, involving distinct contexts and/or distinct kinds of interventions. (Our most complete published account of these issues thus far is &ldquo;A New Causal Power Theory&rdquo; in Korb et al., 2011.)</span></p><p class="c1"><span>Pearl&rsquo;s Do-Calculus reflects this kind of understanding of causal Bayesian networks, and it is perhaps the most widely cited causal interpretation of Bayesian networks in print. It is, however, overly simple. In principle, our CAT, and the causal information theory underlying it, allow for not just &ldquo;overwhelming&rdquo; interventions, where the effects of inbound parent variables are completely rubbed out, as indicated by simply cutting arrows, but also &ldquo;underwhelming&rdquo; interventions, where the causal variable is put into a new distribution that is determined both by the intervention and by the pre-existing parents. </span><span>For an example</span><span>, a medical intervention of strongly advocating a change of lifestyle very often does not result in </span><span>perfect compliance by a patient</span><span>; the old influences persist, if in an attenuated form. Pearl&rsquo;s approach &ndash; aka arc cutting &ndash; simply ignores this reality. (For a discussion, see our paper &ldquo;Varieties of Causal Intervention&rdquo;, Korb et al., 2004.) In this initial version of CAT, however, we anticipate limiting interventions to Pearl&rsquo;s all-or-nothing kind in the interests of completing a Version 1.0 before a </span><span>Version 2.0</span><span class="c3">.</span></p><p class="c1 c26"><span class="c3"></span></p><a id="t.658a013cda56c15c2c4b42b05dc680039c4eb6ef"></a><a id="t.1"></a><table class="c47"><tr class="c15"><td class="c59" colspan="1" rowspan="1"><p class="c12 c25"><span class="c30 c48 c17">CAVEAT: causal versus non-causal networks</span></p><p class="c5"><span class="c3"></span></p><p class="c12"><span>The Bayesian Networks you examine with CAT need to be </span><span class="c9">causal</span><span>&nbsp;Bayesian Networks. Many Bayesian Networks are not causal. For example, Pearl&rsquo;s Alarm network above is causal; however, if we want we can reverse the arc Alarm &rarr; John Calls, i.e., replacing it with Alarm &larr; John Calls, while still representing the </span><span class="c9">very same</span><span>&nbsp;probability distribution. There are rules for doing so, requiring the introduction of additional arrows and reparameterizing the network, which we&rsquo;ll pass over here (for details, see Korb and Nicholson, 2011). However, using those rules, we would end up with the following </span><span class="c9">non</span><span class="c3">-causal Bayesian network:</span></p><p class="c5"><span class="c3"></span></p><p class="c25"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 440.22px; height: 251.43px;"><img alt="" src="images/image6.png" style="width: 440.22px; height: 251.43px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c3"></span></p><p class="c12"><span>As we said, this network is capable of representing the exact same probabilistic relations as the original Alarm network. It can tell the same probabilistic story; however, it cannot tell the same causal story. For example, taken as a causal network, it is </span><span>claiming</span><span>&nbsp;that John&rsquo;s calling Pearl </span><span>causes</span><span>&nbsp;Earthquakes and Burglaries. While that might be an interesting premise for a science fiction story, it&rsquo;s not helpful in understanding </span><span class="c9">our</span><span class="c3">&nbsp;world.</span></p><p class="c5"><span class="c3"></span></p><p class="c12"><span class="c3">CAT takes whatever network you give it and tells you details about the causal story it implies. So, it will happily tell you the science fiction story above. If you want the actual causal story, you need to be sure you&rsquo;ve got the right causal network before you ask CAT about it!</span></p><p class="c5"><span class="c3"></span></p><p class="c12"><span class="c3">As a general rule, it would be better to have an error of omission (a missing arrow) than an arrow of commission, such as a reversed arrow. The first will tell you a simplified causal story, while the second will tell you a wrong causal story.</span></p></td></tr></table><p class="c1 c26"><span class="c3"></span></p><h3 class="c11" id="h.7dvh97tfqmcj"><span>Two </span><span class="c14">Case Studies</span></h3><p class="c1"><span class="c3">We illustrate CAT using two CBNs available with the tool. The first illustration is very simple, while the second goes into more detail. You can, of course, skip either or both and just play around with CAT, but these examples will help most people understand what is going on.</span></p><a id="t.c50c585332072ca25ae66f6e5140354a3173aac6"></a><a id="t.2"></a><table class="c47"><tr class="c15"><td class="c58" colspan="1" rowspan="1"><p class="c12"><span class="c3">A side note on MI calculations in the Causal Information Table: Causal interventions are, as explained above, Pearl&rsquo;s all-or-nothing kind, and they are represented graphically with parent arrows being separated from their effects. Mutual information calculations, however, ignore these interventions, on the ground that MI per se is about probabilistic dependency, not causation. MI calculations are provided for comparison with causal calculations, not as alternative causal calculations. </span></p></td></tr></table><p class="c1 c26"><span class="c3"></span></p><h4 class="c23" id="h.dlilylmx5roa"><span class="c30 c17 c53">Case: Tenure</span></h4><p class="c1"><span>Here we will walk through the simple Tenure example to illustrate the basic features of CAT. After loading the Tenure net, if you hover your cursor above the Tenure variable, you will see the letters &ldquo;C&rdquo; and &ldquo;E&rdquo; above it, for cause and effect. Select &ldquo;E&rdquo;, since we are interested in what influences the chances of getting tenure. A Causal Information Table will be displayed in the </span><span>right margin</span><span>. In the MI column the mutual information (measured in bits) between each variable and the effect variable is shown. Thus, the cell for Tenure shows the mutual information between Tenure and itself; this is the self-information, or entropy, of Tenure. That&rsquo;s the maximum amount of information that can be gained about Tenure given its original (marginal) distribution. The CI column shows variable-to-variable Causal Information with respect to Tenure under a uniform (maxent) distribution, providing a baseline that proportional reductions of entropy can be measured against, reflected in the last (%) </span><span>column</span><span class="c3">. </span></p><p class="c1"><span>Consider </span><span>then</span><span>&nbsp;Hair color. It has an MI of 0.0159 bits with Tenure. That is, there is a probabilistic dependency between having white hair and getting tenure. However, when you consider intervening on Hair (e.g., bleaching it), there is no association. This can be seen already without further manipulation by noting that the CI in the middle column for Hair is 0. The CI reports the dependency between the two variables when Hair is taken as a Cause. You can also see what happens when you manipulate the network. If you observe white hair (by clicking on the label &ldquo;White&rdquo; in the network with only Tenure set to be the Effect), you will find the probability of Tenure increasing. However, if you now set Hair to be a Cause, first the intervention is reflected by its parent arc (from Age_Time) being cut. Also, its distribution becomes uniform. Since it is no longer an observational variable, its priors are irrelevant; the uniform distribution reflects a hypothetical experimental intervention on hair color. </span><span>Note that this kind of intervention has no variable-to-variable effect on Tenure: its distribution is unaffected.</span><span>&nbsp;The Measures table on the right reports what the different causal measures report about the relation between Hair and Tenure. The CI measure reports 0. (MI reports </span><span>0.0159 bits</span><span class="c3">, but that&rsquo;s not actually a causal measure; it&rsquo;s simply again reporting the probabilistic dependency without regard to causality. The other measures are blank; more on that below.)</span></p><p class="c1"><span>Moving to the actual causes for Tenure in the model (ignoring, of course, the many that are </span><span class="c9">not</span><span>&nbsp;being modeled here), we can, for example, compare Age with Academic Record. With </span><span>only Tenure</span><span>&nbsp;set (as the Effect), we can see that </span><span>Age accounts for 16.3%</span><span class="c3">&nbsp;of the variation in Tenure and Academic Record for 7.6%, as measured by variable-to-variable CI. (If this doesn&rsquo;t accord with your intuition about their relative importance, feel free to blame the model; it&rsquo;s a made-up example, after all.) </span></p><p class="c1"><span>If you set Age to Cause, then you get the Measures table with variable-to-variable CI and MI. Aside from the Age distribution resetting to be uniform, you can also look at value-to-variable and value-to-value relations, i.e., digging a little deeper than the top level variable-to-variable CI, we can pull out specific components of the overall variable-to-variable CI calculation. Checking Tenure=Yes (i.e., ticking the box for Tenure=Yes), we can find the power of different specific causes to bring about Tenure (in a way comparable to specifically value-to-value measures, such as Cheng&rsquo;s and FAR, discussed below). Thus, checking Age=Old shows a value-to-value CI of 0.517 bits to produce Tenure (equal to </span><span>51.7% of Tenure&rsquo;s maximum entropy</span><span>, which happens to be 1 bit), with Academic_Record left unobserved. Reversing the roles of these two causes, a Superior Academic Record has a CI of 0.445 bits to produce Tenure (</span><span>44.5%</span><span class="c3">). So in this model, getting Old has a greater power to bring Tenure than improving your Academic performance, but both are substantial influences. </span></p><p class="c1"><span>Some of these value-to-value causal effects may be negative. For example, as one would suspect, being young is a negative factor for being tenured. To see this check Tenure=true; </span><span>then</span><span>&nbsp;Age&rsquo;s variable-to-value CI is about 0.104 bits (or </span><span>10.4%</span><span>). If you now check Age=young, the value-to-value CI is -0.196 (</span><span>-19.6%</span><span>). This negative value represents the negative causal power being young has on getting tenure. (The metaphysics of what&rsquo;s involved </span><span class="c22">&ndash;</span><span>&nbsp;setting your age to be young </span><span class="c22">&ndash;</span><span class="c3">&nbsp;is beyond our philosophical insight; we are reporting only the downstream influences should you acquire such an ability.)</span></p><p class="c1"><span class="c3">The main point of this last exercise is to illustrate that you can pick apart causal influences and examine them separately. You can also examine causal influences under different contexts. For example, if you are interested in the effects of Age on Tenure only for those who have a middling Academic Record, then, without setting Academic Record to be a Cause, simply click on the Middling label, which fixes it as an observation.</span></p><p class="c1"><span class="c3">Note again that individual value-to-value CI can be negative. Whereas variable-level CI measures are necessarily non-negative (as are all MI measures), the individual components (summands) need not be, and when the causal influences on a particular outcome (or subset of outcomes) are negative, they will be. </span></p><p class="c1"><span class="c30 c17 c48">Patricia Cheng&rsquo;s Causal Power (Glymour and Cheng, 1998) and the FAR measure (Stott et al., 2016)</span></p><p class="c1"><span class="c3">Both of these alternative measures are specifically designed to assess value-to-value causal power and not available for variable-to-variable or value-to-variable calculations. That is, they are aimed at explicating such causal locutions as &ldquo;His smoking caused his lung cancer&rdquo;, rather than, say, &ldquo;Smoking is a more significant causal factor for lung cancer than exposure to cosmic rays&rdquo; (this distinction is sometimes described as actual or token causation versus type causation in philosophy). For a proper comparison of these with Causal Information, therefore, it is necessary to restrict CI to value-to-value measures, which describe the contributions of specific causal values to the overall variable-to-variable CI; that is, they report individual summands in the variable-to-variable CI, and thus may be either positive (indicating an inducing factor) or negative (an inhibiting factor). Similarly, both FAR and Cheng&rsquo;s measure can be either positive or negative, reporting either the promotion or inhibition of a specific Effect value of interest. Here, we briefly explain these two measures and compare them with each other and with CI.</span></p><p class="c1"><span>Note that both Cheng&rsquo;s power and FAR assume that the variables involved are binary. We do the same for these measures (and only these measures) by agglomerating any values required. For example, if a user checks Age=Middling, then the contrasting value will be (Age=Young &#8897; Age=Old), with the disjunctive value being treated as a single value. Similarly, CAT will agglomerate multiple checked values in either the Cause or the Effect (by shift-clicking on multiple boxes), treating their disjunction as one value of a binomial variable </span><span>(NB: if </span><span class="c9">all</span><span>&nbsp;the values of either variable are checked, then we have a value-to-variable or variable-to-value case, where FAR and Cheng are no longer valid)</span><span class="c3">.</span></p><p class="c1"><span class="c17">Cheng&rsquo;s measure of Causal Power</span><span class="c3">&nbsp;begins with &ldquo;positive causal contrast&rdquo;:</span></p><p class="c35"><span>&#8710;P</span><span class="c28">c</span><span class="c3">&nbsp;= P(e|c) - P(e|~c) </span></p><p class="c1"><span class="c3">Assuming this is positive, it reports to what extent intervening and setting C=c raises the probability of an effect value of interest, E=e. Cheng proposes using this measure, however, restricted to cases where, without c, e would not have occured; that is, there are cases where e would have occurred anyway, and we wouldn&rsquo;t want to count them toward part of the power of c to produce e. Hence, Cheng&rsquo;s actual formula is:</span></p><p class="c35"><span>p</span><span class="c28">c</span><span class="c3">&nbsp;= ( P(e|c) - P(e|~c) ) / ( 1 - P(e|~c) )</span></p><p class="c1"><span class="c3">which is what she actually calls causal power. An additional constraint is that Cheng and Glymour restrict the use of this measure to Naive Bayes models, where the Cause is the only ancestor to the Effect; i.e., multiple causes are not allowed (or ignored). We enforce a similar restriction by cutting off any parents of the Cause when measuring Cheng&rsquo;s power, although the relevant subnet may not have a Naive Bayes structure (with multiple paths between Cause and Effect allowed). For a detailed explanation, see Glymour and Cheng (1998). Here, we simply compare and contrast the different causal measures, rather than plow into these issues.</span></p><p class="c1"><span class="c17">FAR: </span><span class="c3">Before that, we introduce the last causal power measure considered here: Fraction of Attributable Risk. The FAR measure is:</span></p><p class="c35"><span>FAR(e|c) = 1 - P(e|c</span><span class="c28">0</span><span class="c3">)/P(e|c)</span></p><p class="c1"><span>This is the simplest of the measures we will implement in CAT. Indeed, we consider it far too simple; however, we introduce it because it is in widespread use in epidemiology (where it is a minor variation on &ldquo;risk difference&rdquo;; Wikipedia, 2021), biology and climate science (Stone and Allen, 2005; Stott et al., 2016). The idea behind it is that, where e describes an unusual event (e.g., a weather extreme) and where c</span><span class="c28">0</span><span class="c3">&nbsp;describes a &ldquo;reference state&rdquo; of the world being normal or unperturbed (e.g., a climate without global warming), then the &ldquo;fraction of risk&rdquo; attributable to a perturbation (such as global warming due to CO2, aka &ldquo;c&rdquo;) is the extent to which the C=c pushes up the probability of e, which FAR gives, since it is also equal to (via simple algebra):</span></p><p class="c35"><span>FAR(e|c) = [P(e|c) - P(e|c</span><span class="c28">0</span><span class="c3">)]/P(e|c)</span></p><p class="c1"><span>Note that c</span><span class="c28">0</span><span>&nbsp;is not </span><span class="c9">necessarily</span><span>&nbsp;~c, but simply some reference state alternative to C=c. Since we are approaching FAR through the lens of the climate attribution literature, where c = anthropenic global warming and c</span><span class="c28">0</span><span>&nbsp;= the absence of anthropenic global warming (both as reflected in global circulation models generating the estimated probabilities of the extreme event e), CAT treats c</span><span class="c28">0</span><span class="c3">&nbsp;as ~c.</span></p><p class="c1"><span>A question arises if the ratio P(e|c</span><span class="c28">0</span><span>)/P(e|c) happens to be greater than 1. A simple use of the FAR definition would yield a negative number, which is not interpretable as a fraction. Stone and Allen (2005, &sect;2.1) give two conflicting responses. Initially, they bite the bullet: in these cases &ldquo;no attribution can be made to an increase in the risk of the event occurring, since the risk did not increase.&rdquo; The risk </span><span class="c9">changed</span><span>, however, and it seems that the very same questions arise about what was responsible for the change and how much. So, they fall back to accepting what the attribute to epidemiological discussions of risk, swapping the roles of &ldquo;c&rdquo; and &ldquo;c</span><span class="c28">0</span><span>&ldquo; (so, the FAR number is positive), but report it as preventative. Thus, if some extreme event (say, an extreme cold snap) should become </span><span class="c9">less</span><span>&nbsp;likely under global warming, then the positive fraction computed from putting non-global warming in the denominator in the definition above will be reported as a preventative fraction. This is what we&rsquo;ve implemented in FAR, since it is likely more helpful than simply reporting &ldquo;no FAR exists&rdquo;.</span></p><p class="c1 c26"><span class="c3"></span></p><a id="t.a9ba9faea308b9053fce2e5467790af463c163c7"></a><a id="t.3"></a><table class="c47"><tr class="c15"><td class="c27" colspan="1" rowspan="1"><p class="c12"><span>Actually, the &ldquo;insight&rdquo; that the world is binary is part of the oversimplification of both </span><span>FAR</span><span>&nbsp;and Cheng&rsquo;s causal power. It is more than common that a causal factor has many distinct states, and that we should like to assess their powers separately and in comparison with each other. A way to do that with these binary measures is to rotate through all the distinct causal values, taking each in turn as &ldquo;c&rdquo;, while combining all other states into a new &ldquo;~c&rdquo;. This is rather crude, however, and subject to statistical oddities, such as </span><span class="c18"><a class="c8" href="https://plato.stanford.edu/entries/paradox-simpson/">Simpson&rsquo;s Paradox</a></span><span class="c3">.</span></p></td></tr></table><p class="c1 c26"><span class="c3 c20"></span></p><p class="c1"><span class="c30 c48 c17">Comparison of Cheng&rsquo;s Causal Power, FAR and CI</span></p><p class="c1"><span>So, let&rsquo;s compare causal powers. </span><span>For example, we might wonder about the causal power of a superior academic record to bring about tenure. In this case, making Academic Record be the Cause and checking it to be Superior while simultaneously checking Tenure=Yes, the Cheng measure reports a power of Superior Academic Record for bringing about tenure to be 0.53, while FAR reports an attribution of 47% of a tenured outcome to a Superior Record. The value-to-value Causal Information is also, of course, positive, namely 0.44 bits. All measures </span><span>are go</span><span class="c3">. Again, we can look at the causal power of bleaching your hair white, when we get, unsurprisingly, causal power measures of 0, 0 and 0, for CI, Cheng&rsquo;s power and FAR respectively. </span></p><p class="c1"><span>Of more interest, certainly, is finding cases where these three measures diverge and figuring out why they diverge. That sort of investigation is one motive behind developing CAT, but carrying it out is beyond the scope of this Explainer. Readers are invited to do their own </span><span>investigating</span><span>.</span></p><h4 class="c23" id="h.xci757yv40l1"><span class="c30 c53 c17">Case: Coronary Risk Assessment</span></h4><p class="c1"><span>We</span><span>&nbsp;continue this Explainer instead with a somewhat more complicated example, </span><span class="c18"><a class="c8" href="https://www.norsys.com/netlibrary/index.htm">Coronary Risk</a></span><span>&nbsp;(from Assessment Technologies, Inc. NB: Their original network is continuous, but our BN engine is limited to discrete networks; hence, this and any other continuous nets loaded into CAT must first be discretized. Other differences result from converting it into a causal network, since the original is non-causal.), downloaded from </span><span class="c18"><a class="c8" href="https://www.norsys.com/">Norsys</a></span><span>,</span><span>&nbsp;illustrating additional features of CAT when using the causal information measure. (For a </span><span>defence</span><span class="c3">&nbsp;and explanation of that measure see Korb, Nyberg and Hope, 2011.) You should follow this discussion by loading the network into CAT yourself and performing the operations we describe. (We won&rsquo;t reproduce diagrams for each operation.)</span></p><p class="c1"><span class="c3">The Coronary Risk network is (without any observations, etc.):</span></p><p class="c35"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 663.14px; height: 448.00px;"><img alt="" src="images/image5.png" style="width: 663.14px; height: 448.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span>These are the marginal probabilities of the variables, i.e., without either interventions or observations for the given population (who are people from </span><span class="c18"><a class="c8" href="https://en.wikipedia.org/wiki/Framingham_Heart_Study">the Framingham heart study</a></span><span>&nbsp;</span><span>from 1950 to 1990). will focus on the following variables (for an explanation of variables plus publication references, see the so-called &ldquo;</span><span class="c18"><a class="c8" href="https://www.norsys.com/netlibrary/nets/tut/Coronary%20Risk_tut.htm">tutorial</a></span><span class="c3">&rdquo; at Norsys):</span></p><ul class="c36 lst-kix_3obzojhpo3da-0 start"><li class="c1 c2 li-bullet-0"><span class="c3">Systolic Blood Pressure (where &ldquo;Stage 3&rdquo; is the highest)</span></li><li class="c1 c2 li-bullet-0"><span class="c3">Age</span></li><li class="c1 c2 li-bullet-0"><span class="c3">Smoking status</span></li><li class="c1 c2 li-bullet-0"><span class="c3">Coronary Artery Disease, or CAD (the target variable of interest, where the worst value is CAD_to_10 and the best CAD_90_to_100)</span></li></ul><p class="c1"><span>If you set CAD to be the Effect variable, you can examine the Causal Information Table, which includes the marginal Mutual Information. Each of the first three (causal) variables above has a potential observational value which raises the probability of Coronary Artery Disease (CAD), the target variable of interest, being in its worst state (i.e., CAD_to_10)</span><span class="c3">; go ahead and find those states by clicking on alternatives (not the boxes, but the values directly).</span></p><p class="c1"><span>If you set the three variables Systolic BP, Age and Smoking simultaneously to their most problematic values (Smoking = true, etc.), the probability of CAD = [0,10] rises to 1.5%, which is much higher than the marginal (prior to evidence) probability of 0.0. (For simplicity, percentages or chances here are referred to as probabilities, since that is how the tool represents things.) All of the probabilities shown by the model so far are &ldquo;observational probabilities&rdquo;, i.e., probabilities conditioned upon some observations, except for those values that you actually set (since they </span><span class="c9">are</span><span class="c3">&nbsp;the observations), which of course take the post-observation value of 100%. </span></p><a id="t.5356f0c0b8b89a7c9e32b4803086801aca001efa"></a><a id="t.4"></a><table class="c47"><tr class="c15"><td class="c49" colspan="1" rowspan="1"><p class="c12"><span>Note that observations are indicated by setting a particular value of the observed variable. That&rsquo;s done by clicking on the value label in the variable box. E.g., to set Age=20_to_45 just click on &ldquo;s20to45&rdquo;; the bar on the right should then extend to 100%. Partial observations, where the probabilities of specific values are higher than 0 but less than 100, are not currently supported, other than for the uniform distribution. We may support partial observation in the future (otherwise known as </span><span class="c18"><a class="c8" href="https://plato.stanford.edu/entries/epistemology-bayesian/%23sec-jeffrey">Jeffrey Conditionalization</a></span><span class="c3">).</span></p><p class="c5"><span class="c3"></span></p><p class="c12"><span>On the other hand, CAT does support partial selections for Causal Information calculations specifically, by checking one or more boxes to the left of the labels (for more than one box, use shift-click). These shouldn&rsquo;t be thought of as observations, but just requests for the component CI computations corresponding to those values.</span></p></td></tr></table><p class="c1 c26"><span class="c3"></span></p><p class="c1"><span>What we want to know for causal attribution is how much of the variation in the (target) variable of interest, CAD, is explained by potential variations, or changes, in the causes of interest. The CI table with CAD set as the Effect is how we can examine this kind of causal responsibility. With observation values set, those observed variables lose their variability, however, so CI entries for them drop to zero. Unsetting them (clicking on the value labels again) reveals the variable-to-variable CI for CAD, which are 6.1</span><span>%, 3.7% and 0.4%</span><span class="c3">&nbsp;for BP, Age and Smoking respectively. These are the variable-to-variable causal responsibilities for coronary arterial disease, not simply associations, so Causal Information Theory asserts BP is playing the largest role of the three.</span></p><p class="c1"><span class="c3">It&rsquo;s worth noting, by the way, that these (and all) probabilities in the model are sensitive to the population being modeled. Conclusions can only be drawn for that population and not (directly) for any other population. If the demographics of some other population differ significantly from those of the Framingham study, then extrapolation to them will be problematic. </span></p><p class="c1"><span>If you take these variables in turn as causes (by clicking on &ldquo;C&rdquo; for each in turn), the reported CI value for each will be unchanged, because they are reporting the same thing, variable-to-variable CI. What will change are: any inbound arcs will be &ldquo;cut&rdquo;, because the effects of a cause are presumed to be independent of the cause&rsquo;s ancestors (go ahead and set Smoking to &ldquo;C&rdquo; to see this); the selected distribution will change from the prior distribution for that variable to the uniform distribution, to represent a uniform causal intervention on that variable. </span><span class="c34">Also, the CI for parents of the Cause variable will change, since the path through the Cause has been cut. </span></p><a id="t.75874ffc821c3706ef68cef5145c19300a053c3a"></a><a id="t.5"></a><table class="c47"><tr class="c15"><td class="c60" colspan="1" rowspan="1"><p class="c12"><span class="c3">NB: In the current CAT version the uniform distribution is the only available causal intervention distribution. The uniform distribution corresponds to how many randomized experiments are performed, by giving equal numbers of a test population either an experimental dose of a medicine or a placebo, for example. There are other distributions that might make sense, to be sure, as in proportional stratified sampling, or just selecting some specific distribution for CI. These are possibilities for the future.</span></p><p class="c5"><span class="c3"></span></p><p class="c12"><span>To be sure, CAT currently does allow you to select a subset of specific values for CI calculations of value-to-variable, variable-to-value or value-to-value (causal power) computations; this is done by box ticking (&ldquo;checking&rdquo; in this Explainer), when unselected values disappear and the uniform distribution is redistributed across only the selected values. For example, if you set Age as a Cause, its explanatory percent is 3.7%; if you then specify 20-45 as the age range, CI adjusts to 8</span><span>.2</span><span>%, since other values for age become impossible (and CAD&rsquo;s entropy has decreased). Adding each older group in turn (extending the selected range from 20-45 to 20-55 and then 20-65 and finally 20-75, by shift-clicking) yields 4</span><span>.3%, 3.4%</span><span class="c3">&nbsp;and, with all four values selected, back to 3.7%.</span></p></td></tr></table><p class="c1 c26"><span class="c3"></span></p><p class="c1"><span>Suppose we are interested in the three worst categories of CAD, that is, </span><span>the range 0-30.</span><span>&nbsp;(Perhaps unintuitively, CAD discrete values are ordered from worst cases to normal.) Then, we can set CAD as the Effect and </span><span>check the first three values</span><span>, restricting the causal power measures to them. If we compare the worst values for BP, Age and Smoking in turn, we get CI measures of </span><span>0.082, 0.021 and 0.003</span><span class="c3">&nbsp;respectively, indicating that having high blood pressure is the most important causal factor for higher risk of CAD. Qualitatively the same stories are told by Cheng and FAR, with measures 0.463, 0.014 and 0.003 for Cheng and 0.882, 0.64 and 0.365 for FAR.</span></p><p class="c1"><span>The qualitative story may be important (we won&rsquo;t go beyond &ldquo;may&rdquo; without first doing serious research on the subject of coronary artery disease!), but the quantitative details will always be problematic. We note above that the numbers depend upon the population studied, but they also depend upon many other factors, such as how the variables are </span><span>discretized</span><span>. If you choose an </span><span>oldest</span><span class="c3">&nbsp;Age range of 60-75 instead of 65-75, then the disease rates will differ, of course. What CAT tells you is that in some fanciful world that approximates our own, but one where Age has the specified four discrete values and one instantaneously progresses from one age to the next, then the rates of disease are such-and-such. CAT delineates the causal relations in that world quantitatively. If you are dissatisfied with the stories it tells, you can try to extend and expand your model (e.g., introduce finer discrete categories), or to better parameterize it with more data.</span></p><p class="c1"><span>Simply selecting a target variable already gets CAT to tell you interesting facts about the target&rsquo;s variability and how other variables can reduce that variability when observed (mutual information) or when they are intervened upon (causal information). Furthermore, you can inspect individual causal values, or groups of values, to see what causal power they have to bring about a specific effect, or range of effects, using CI, Cheng&rsquo;s power and FAR. Which of these yield the best explanatory account of causation in an individual case, we leave to a future paper (or to you).</span></p><h3 class="c11" id="h.d2uijh4r1gkm"><span class="c14">Appendix: Varieties of Causal Criteria</span></h3><p class="c1"><span class="c3">The following criteria have been proposed in the literature and we would think worthwhile implementing on our platform; we intend to implement at least some for comparative purposes (blue indicates criteria that have been implemented):</span></p><ol class="c36 lst-kix_mz0aqo6a63b1-0 start" start="1"><li class="c0 li-bullet-0"><span class="c16">Causal Information Theoretic Criterion</span><span class="c3">&nbsp;(Korb, Nyberg and Hope, 2011). This is our own. We briefly explain it above. </span></li><li class="c0 li-bullet-0"><span class="c16">Fraction of Attributable Risk</span><span>&nbsp;(aka FAR; Stott, et al., 2016). This is widely used in environmental sciences as well as global warming event attribution. </span></li><li class="c0 li-bullet-0"><span class="c17">Halpern-Pearl&rsquo;s Criterion for Actual Causation </span><span class="c3">(Halpern and Pearl, 2005a; Halpern and Pearl, 2005b). This is a non-probabilistic (i.e., strictly graphical) criterion. Since it requires a good deal of hacking the CBN, as well as an exponential search, this will not be implemented soon.</span></li><li class="c0 li-bullet-0"><span class="c17">Twardy and Korb&rsquo;s Probabilistic Halpern-Pearl Criterion</span><span class="c3">&nbsp;(Twardy and Korb, 2004). A probabilistic version of the Halpern-Pearl Criterion.</span></li><li class="c0 li-bullet-0"><span class="c17">Hitchcock&rsquo;s Criterion of Actual Causation</span><span class="c3">&nbsp;(Hitchcock, 2001). A simpler non-probabilistic criterion in the spirit of Halpern and Pearl&rsquo;s.</span></li><li class="c0 li-bullet-0"><span class="c17">Pearl&rsquo;s Do-Calculus </span><span class="c3">(Pearl, 2000). A non-probabilistic, graphical criterion.</span></li><li class="c0 li-bullet-0"><span class="c17">Lewis&rsquo;s Counterfactual Criterion of Causation</span><span class="c3">&nbsp;(Lewis, 2004). While Lewis does not make reference to graphical structure, there are plausible accounts of counterfactuality using causal models which would allow for an implementation of some version of Lewis&rsquo;s theory.</span></li><li class="c0 li-bullet-0"><span class="c17 c34">Cheng&rsquo;s original Causal Power Theory </span><span class="c3">(Cheng, 1997). Cheng&rsquo;s account is limited to a restricted class of causal Bayesian networks, but is easily implemented for that class.</span></li><li class="c0 li-bullet-0"><span class="c16">Cheng &amp; </span><span class="c16">Glymour&rsquo;s Causal</span><span class="c16">&nbsp;Power Theory</span><span class="c38">&nbsp;</span><span class="c3">(Glymour and Cheng, 1998). A modification of Cheng&rsquo;s account.</span></li><li class="c0 li-bullet-0"><span class="c17">Good&rsquo;s Causal Calculus </span><span class="c3">(Good, 1961). An account of causal power restricted to linear networks.</span></li><li class="c0 li-bullet-0"><span class="c17">Wright&rsquo;s Path Model Criterion</span><span class="c3">&nbsp;(Wright, 1934). The original graphical measure of causal power. It&rsquo;s limited to linear models, but is easy to implement for such cases. (We do not anticipate supporting linear models any time soon, however.)</span></li></ol><p class="c1"><span class="c3">If you are aware of an interesting causal criterion which would be worthwhile implementing, then please either implement it and notify us, or at least bring it to our attention.</span></p><h3 class="c11" id="h.284nmetkcsm0"><span class="c14">Note on Source Code</span></h3><p class="c1"><span>The CAT source code is available at:</span><span>&nbsp;</span><span class="c18 c41"><a class="c8" href="https://github.com/voracity/CAT">https://github.com/voracity/CAT</a></span><span>.</span></p><h3 class="c11" id="h.9gul6g2wjhw5"><span class="c14">Further Reading</span></h3><p class="c1"><span class="c7">Cheng, P. W. (1997). From covariation to causation: A causal power theory. </span><span class="c4">Psychological review, 104(2),</span><span class="c13 c7">&nbsp;367.</span></p><p class="c1"><span class="c7">Cover, T. M., &amp; Thomas, J. A. (2006). </span><span class="c4">Elements of Information Theory</span><span class="c7">, 3rd edition. </span><span class="c13 c7">Wiley Series in Telecommunications and Signal Processing.</span></p><p class="c1"><span class="c7">Good, I. J. (1961). A causal calculus. </span><span class="c4">British Journal for the Philosophy of Science 11, </span><span class="c13 c7">305&ndash;318.</span></p><p class="c1"><span class="c7">Glymour, C. and P. Cheng (1998). Causal mechanism and probability: a normative approach. In M. Oaksford and N. Chater (Eds.), </span><span class="c4">Rational models of cognition</span><span class="c13 c7">. Oxford: Oxford Univ. Press.</span></p><p class="c1"><span class="c7">Halpern, J. Y., &amp; Pearl, J. (2005a). Causes and Explanations: A Structural-Model Approach. Part I: Causes. </span><span class="c4">British Journal for the Philosophy of Science, 56(4),</span><span class="c13 c7">&nbsp;843-843.</span></p><p class="c1"><span class="c7">Halpern, J. Y., &amp; Pearl, J. (2005b). Causes and Explanations: A Structural-Model Approach. Part II: Explanations. </span><span class="c4 c43 c42">British Journal for the Philosophy of Science, 56(4).</span></p><p class="c1"><span class="c7">Handfield, T., Twardy, C. R., Korb, K. B., &amp; Oppy, G. (2008). The metaphysics of causal models. </span><span class="c4">Erkenntnis, 68(2),</span><span class="c13 c7">&nbsp;149-168.</span></p><p class="c1"><span class="c7">Hitchcock, C. (2001). The intransitivity of causation revealed in equations and graphs. </span><span class="c4">The Journal of Philosophy, 98(6), </span><span class="c13 c7">273-299.</span></p><p class="c1"><span class="c7">Hitchcock, Christopher, &quot;Probabilistic Causation&quot;, The Stanford Encyclopedia of Philosophy (Spring 2021 Edition), Edward N. Zalta (ed.), URL = &lt;</span><span class="c18 c40 c37"><a class="c8" href="https://plato.stanford.edu/archives/spr2021/entries/causation-probabilistic/">https://plato.stanford.edu/archives/spr2021/entries/causation-probabilistic/</a></span><span class="c13 c7">&gt;.</span></p><p class="c1"><span class="c7">Korb, K. (2020). </span><span class="c18 c37 c40"><a class="c8" href="https://eartharxiv.org/repository/object/95/download/187/">How Extreme Weather Events Are Attributed to Anthropogenic Global Warming. </a></span><span class="c4">Eartharxiv.org</span><span class="c4 c42 c43">.</span></p><p class="c1"><span class="c7">Korb, K. B., Hope, L. R., Nicholson, A. E., &amp; Axnick, K. (2004). Varieties of causal intervention. </span><span class="c7">In </span><span class="c4">Pacific</span><span class="c4">&nbsp;Rim International Conference on Artificial Intelligence</span><span class="c13 c7">&nbsp;(pp. 322-331). Springer, Berlin, Heidelberg.</span></p><p class="c1"><span class="c7">Korb, K. B., &amp; Nicholson, A. E. (2011). </span><span class="c4">Bayesian artificial intelligence, </span><span class="c7">2nd edition.</span><span class="c4">&nbsp;</span><span class="c13 c7">CRC press.</span></p><p class="c1"><span class="c7">Korb, K. B., Nyberg, E. P., &amp; Hope, L. (2011). </span><span class="c7">A new causal power theory. In Illari, Russo and Williamson (Eds) </span><span class="c4">Causality in the Sciences</span><span class="c13 c7">, Oxford University Press, pp. 628-652.</span></p><p class="c1"><span class="c7">Lewis, D. (2004). Causation as influence. </span><span class="c4">The Journal of Philosophy, 97(4), </span><span class="c13 c7">182-197.</span></p><p class="c1"><span class="c7">Pearl, J. (1988). </span><span class="c4">Probabilistic reasoning in intelligent systems: networks of plausible inference. </span><span class="c7 c13">Morgan Kaufmann.</span></p><p class="c1"><span class="c7">Pearl, J. (2000). </span><span class="c4">Causality</span><span class="c13 c7">. Cambridge University Press.</span></p><p class="c1"><span class="c37">Stone, D. &amp; Allen, M. R. &nbsp;(2005) </span><span class="c37">The End-To-End Attribution Problem: From Emissions To Impacts. </span><span class="c37 c39">Climatic Change, 71, </span><span class="c37">303&ndash;318.</span></p><p class="c32"><span class="c7">Stott, P. A., Christidis, N., Otto, F. E., Sun, Y., Vanderlinden, J. P., Van Oldenborgh, G. J., &hellip; &amp; Zwiers, F. W. (2016). Attribution of extreme weather and climate&#8208;related events. </span><span class="c4">Wiley Interdisciplinary Reviews: Climate Change, 7(1)</span><span class="c13 c7">, 23-41.</span></p><p class="c1"><span class="c7">Twardy, C. R., &amp; Korb, K. B. (2004). A criterion of probabilistic causation. </span><span class="c4">Philosophy of Science, 71(3),</span><span class="c13 c7">&nbsp;241-262.</span></p><p class="c1"><span class="c7">Wikipedia. (2021, October 16). Risk difference. In Wikipedia, The Free Encyclopedia. Retrieved 06:19, June 1, 2022, from </span><span class="c18 c40 c37"><a class="c8" href="https://en.wikipedia.org/w/index.php?title%3DRisk_difference%26oldid%3D1050239232">https://en.wikipedia.org/w/index.php?title=Risk_difference&amp;oldid=1050239232</a></span></p><p class="c1"><span class="c7">Woodward, J. (2005). </span><span class="c4">Making things happen: A theory of causal explanation</span><span class="c13 c7">. Oxford University Press.</span></p><p class="c1"><span class="c7">Wright, S. (1934). The method of path coefficients. </span><span class="c4">Annals of Mathematical Statistics 5, </span><span class="c13 c7">161&ndash;215.</span></p><p class="c1 c26"><span class="c13 c7"></span></p>